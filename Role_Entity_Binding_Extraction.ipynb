{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pks2906/Role-Entity-Binding-Extraction/blob/main/Role_Entity_Binding_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 1\n"
      ],
      "metadata": {
        "id": "xNO4DLRb8xvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuqeKXRg5q3P",
        "outputId": "b8b0d296-b209-4e62-a9c5-49d290c06a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what has NN VBN TO NN', 'who has VBN NN TO NN', 'what has been VBN TO NN', 'did NN VB NN Time', 'from whom do NN VB NN', 'TO whom has NN VBN NN', 'NN has VBN NN', 'NN has VBN NN by NN', 'NN has VBN DT NN', 'NN has VBN NN from NN', 'NN VBN RB RB in DT event', 'who VBN NN by NN', 'NN VBN NN', 'NN VBZ RB RB', 'NN VBZ NN of NN in DT place', 'why did NN NN NN', 'where DT NN had VBN', 'who VBN DT NN', 'who did RB VBN in DT event', 'why NN VBN IN NN', 'where NN VBN DT NN', 'what has been VBN in DT NN', 'why has NN VBN NN TO NN', 'at what NN NN VBN NN TO NN', 'when has NN VBN NN TO NN', 'when did NN VB', 'from whom did NN VB NN', 'from where did NN VB NN', 'why NN did RB VBN IN NN', 'who has VBN NN', 'who VBN NN', 'who VBN NN', 'who VBN NN', 'did NN VB NN', 'what has been VBN TO VB', 'do NN VB NN', 'from where did NN VB NN', 'why did NN VB NN', 'have NN VBN NN', 'has NN VBN NN', 'WP VBN NN', 'when did NN VB NN', 'what did NN VB TO NN', 'when have NN VBN NN', 'what did VB TO NN', 'what has been VBN TO NN', 'when NN VBZ', 'what did NN VBP TO NN', 'what did NN VB', 'what has NN VBN', 'what has been VBN by NN', 'what NN have NN VBP in event', 'what NN did NN VB at event', 'what has been VBN at event', 'what has NN VBN TO NN', 'when did NN NN NN', 'when has NN VBN NN', 'what did NN VB TO NN']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import numpy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "list=['what', 'when', 'whom', 'why', 'where','who','been','has','had','have','did','do','does', 'from','in','at','onto','by','of']\n",
        "list1=['sita','ram', 'shyam']\n",
        "list3=['yesterday','today','tomorrow']\n",
        "list4=['school','home','house']\n",
        "list5=['party','festival','function']\n",
        "sentence=[\"what has ram given to sita\",\n",
        "          \"who has given book to sita\",\n",
        "          \"what has been given to sita\",\n",
        "          \"did you call me yesterday\",\n",
        "          \"from whom do you take books\",\n",
        "          \"to whom has ram given books\",\n",
        "          \"ram has hitted shyam\",\n",
        "          \"ram has hitted shyam by stone\",\n",
        "          \"ram has given all books\",\n",
        "          \"sita has taken books from library\",\n",
        "          \"shyam danced very well in the party\",\n",
        "          \"who hitted shyam by stone\",\n",
        "          \"ram hitted shyam\",\n",
        "          \"ram runs very fast\",\n",
        "          \"ram makes fun of shyam in the school\",\n",
        "          \"why did ram kill ravan\",\n",
        "          \"where the battle had held\",\n",
        "          \"who opened the fridge\",\n",
        "          \"who did not come in the party\",\n",
        "          \"why you shouted on shyam\",\n",
        "          \"where you kept the keys\",\n",
        "          \"what has been kept in the box\",\n",
        "          \"why has ram given books to shyam\",\n",
        "          \"at what time ram given books to sita\",\n",
        "          \"when has ram given books to sita\",\n",
        "          \"when did ram come\",\n",
        "          \"from whom did sita take books\",\n",
        "          \"from where did sita take books\",\n",
        "          \"why ram did not come on sunday\",\n",
        "          \"who has given book\",\n",
        "          \"who killed ravan\",\n",
        "          \"who given books\",\n",
        "          \"who touched bottle\",\n",
        "          \"did you call mohan\",\n",
        "          \"what has been given to mohan\",\n",
        "          \"do you eat apple\",\n",
        "          \"from where did you get books\",\n",
        "          \"why did you get books\",\n",
        "          \"have you taken books\",\n",
        "          \"has he joined college\",\n",
        "          \"Who  scolded you\",\n",
        "          \"when did she scold you\",\n",
        "          \"what did you give  to ram\"\n",
        "          ,\"when have you given books\" ,\n",
        "          \"what did happen to ram\",\n",
        "          \"what has been taught to ram\",\n",
        "          \"when train  comes\",\n",
        "          \"what did you say to ram\",\n",
        "          \"what did ram say\",\n",
        "          \"what has ram given\",\n",
        "          \"what has been given by ram\",\n",
        "          \"what dish have you ate in party\",\n",
        "          \"what game did you play at party\",\n",
        "          \"what has been played at party\",\n",
        "          \"what has ram given to shyam\",\n",
        "          \"when did ram scold shyam\",\n",
        "          \"when has ram scolded shyam\",\n",
        "          \"what did ram give to sita\"]\n",
        "y=len(sentence)\n",
        "arr2=[]\n",
        "for j in range(y):\n",
        "  word=nltk.word_tokenize(sentence[j])\n",
        "  word1=nltk.pos_tag(word)\n",
        "  x=len(word1)\n",
        "  arr = numpy.array(word1)\n",
        "  [s,s2]=numpy.shape(arr)\n",
        "  for t in range(s):\n",
        "    if arr[t,0] in list:\n",
        "      arr[t,1]=arr[t,0]\n",
        "    elif arr[t,0] in list1:\n",
        "      arr[t,1]='NN'\n",
        "    elif arr[t,1] == 'NNS':\n",
        "      arr[t,1]='NN'\n",
        "    elif arr[t,1] =='PRP':\n",
        "      arr[t,1]='NN'\n",
        "    elif arr[t,0] in list3:\n",
        "      arr[t,1]='Time'\n",
        "    elif arr[t,1]=='VBD':\n",
        "      arr[t,1]='VBN'\n",
        "    elif arr[t,0] in list4:\n",
        "      arr[t,1]='place'\n",
        "    elif arr[t,0] in list5:\n",
        "      arr[t,1]='event'\n",
        "\n",
        "\n",
        "  arr1=arr[:,1]\n",
        "  ''.join(arr1)\n",
        "  arr2.append(' '.join(arr1))\n",
        "print(arr2)\n",
        "len(arr2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=type(sentence[1])\n",
        "y"
      ],
      "metadata": {
        "id": "-YIKYJf2eE5f",
        "outputId": "f27ace54-0095-4e21-d7a3-e09afb9cb772",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHEhuRG-hQ8W",
        "outputId": "36c61fc1-afcf-4ced-aa21-31bf5b573637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.preprocessing.text.one_hot\n",
        "count=0\n",
        "unique=[]\n",
        "for i in arr2:\n",
        "  j=i.split()\n",
        "  for k in j:\n",
        "    if k not in unique:\n",
        "      unique.append(k)\n",
        "      count=count+1\n",
        "print(count)\n",
        "sentence_length=[]\n",
        "for k in arr2:\n",
        "  count=0\n",
        "  j=k.split()\n",
        "  for i in j:\n",
        "    count=count+1\n",
        "  sentence_length.append(count)\n",
        "\n",
        "max(sentence_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX-FG4TkNyAz",
        "outputId": "36788756-1207-44a6-aad4-2ca52fd9dac0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "word_dict={}\n",
        "count=0\n",
        "for i in arr2:\n",
        "  k=i.split()\n",
        "  for l in k:\n",
        "    if l not in word_dict:\n",
        "      word_dict[l]=count\n",
        "      count=count+1\n",
        "len(word_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8FSQ5rlQ6Vw",
        "outputId": "6bc7f856-979b-49cc-da7d-443f8614cb99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58, 8, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "import numpy as np\n",
        "arr2_vec=np.zeros((58,8,30),dtype='int')\n",
        "count1=0\n",
        "for i in arr2:\n",
        "  k=i.split()\n",
        "  count=0\n",
        "  for l in k:\n",
        "    arr2_vec[count1,count,word_dict[l]]=1\n",
        "    count=count+1\n",
        "  count1=count1+1\n",
        "arr2_vec.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "dUNdc57dG9Ki"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import legacy\n",
        "\n",
        "optimizer = legacy.Adam(lr=0.001, decay=1e-19)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "eZG4BKh8HAxC"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF51P0JLVCmq",
        "outputId": "244c7035-3da7-41b7-d04b-bc067832080b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_8 (LSTM)               (None, 64)                24320     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                1040      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25360 (99.06 KB)\n",
            "Trainable params: 25360 (99.06 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "model=Sequential()\n",
        "model.add(layers.Input(shape=(arr2_vec.shape[1:])))\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(16, activation='sigmoid'))\n",
        "#model.compile( loss='categorical_crossentropy',\n",
        "              #optimizer = legacy.Adam(lr=0.001, decay=1e-19)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "#Fitting the data to the model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming arr2_vec has more samples\n",
        "arr2_vec = arr2_vec[:output_mat.shape[0]]\n"
      ],
      "metadata": {
        "id": "g5x1XK6iIace"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming output_mat has more samples\n",
        "output_mat = output_mat[:arr2_vec.shape[0]]\n"
      ],
      "metadata": {
        "id": "o4pCFI-fIdWl"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P9o4-PbQIex1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(arr2_vec, output_mat, epochs=481)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiEmATgED86d",
        "outputId": "e887b8ef-126b-4ca1-8a9f-726c61a9c57c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/481\n",
            "2/2 [==============================] - 4s 26ms/step - loss: 2.7665 - accuracy: 0.0577\n",
            "Epoch 2/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2.7370 - accuracy: 0.3077\n",
            "Epoch 3/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.7060 - accuracy: 0.4231\n",
            "Epoch 4/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.6761 - accuracy: 0.5385\n",
            "Epoch 5/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2.6451 - accuracy: 0.6346\n",
            "Epoch 6/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2.6031 - accuracy: 0.6154\n",
            "Epoch 7/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.5575 - accuracy: 0.5769\n",
            "Epoch 8/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.5076 - accuracy: 0.5962\n",
            "Epoch 9/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.4465 - accuracy: 0.5192\n",
            "Epoch 10/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2.3566 - accuracy: 0.5577\n",
            "Epoch 11/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.2626 - accuracy: 0.5577\n",
            "Epoch 12/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.1016 - accuracy: 0.5769\n",
            "Epoch 13/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.9419 - accuracy: 0.5577\n",
            "Epoch 14/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.6984 - accuracy: 0.5769\n",
            "Epoch 15/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.4728 - accuracy: 0.5000\n",
            "Epoch 16/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.1474 - accuracy: 0.5962\n",
            "Epoch 17/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.9156 - accuracy: 0.5385\n",
            "Epoch 18/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8368 - accuracy: 0.5577\n",
            "Epoch 19/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.7501 - accuracy: 0.5000\n",
            "Epoch 20/481\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.7556 - accuracy: 0.5192\n",
            "Epoch 21/481\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.7215 - accuracy: 0.4808\n",
            "Epoch 22/481\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.7299 - accuracy: 0.5577\n",
            "Epoch 23/481\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.7361 - accuracy: 0.5192\n",
            "Epoch 24/481\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.7119 - accuracy: 0.5192\n",
            "Epoch 25/481\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.7214 - accuracy: 0.3846\n",
            "Epoch 26/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.7424 - accuracy: 0.4615\n",
            "Epoch 27/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.7475 - accuracy: 0.4615\n",
            "Epoch 28/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7352 - accuracy: 0.4615\n",
            "Epoch 29/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.6858 - accuracy: 0.5962\n",
            "Epoch 30/481\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.7582 - accuracy: 0.4423\n",
            "Epoch 31/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7395 - accuracy: 0.4615\n",
            "Epoch 32/481\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.7265 - accuracy: 0.5769\n",
            "Epoch 33/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7357 - accuracy: 0.5577\n",
            "Epoch 34/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7101 - accuracy: 0.5769\n",
            "Epoch 35/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7417 - accuracy: 0.5000\n",
            "Epoch 36/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.7009 - accuracy: 0.5769\n",
            "Epoch 37/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.7768 - accuracy: 0.4423\n",
            "Epoch 38/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.7794 - accuracy: 0.4038\n",
            "Epoch 39/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.7327 - accuracy: 0.4038\n",
            "Epoch 40/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.6833 - accuracy: 0.5962\n",
            "Epoch 41/481\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.7795 - accuracy: 0.4038\n",
            "Epoch 42/481\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.7339 - accuracy: 0.5192\n",
            "Epoch 43/481\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.6781 - accuracy: 0.5385\n",
            "Epoch 44/481\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.6988 - accuracy: 0.5577\n",
            "Epoch 45/481\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.7105 - accuracy: 0.5769\n",
            "Epoch 46/481\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.6454 - accuracy: 0.6731\n",
            "Epoch 47/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.8115 - accuracy: 0.3654\n",
            "Epoch 48/481\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.6708 - accuracy: 0.5000\n",
            "Epoch 49/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7287 - accuracy: 0.5000\n",
            "Epoch 50/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.7727 - accuracy: 0.4423\n",
            "Epoch 51/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7603 - accuracy: 0.3846\n",
            "Epoch 52/481\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.6749 - accuracy: 0.5385\n",
            "Epoch 53/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6548 - accuracy: 0.6154\n",
            "Epoch 54/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.7458 - accuracy: 0.4808\n",
            "Epoch 55/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7455 - accuracy: 0.4423\n",
            "Epoch 56/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7049 - accuracy: 0.5000\n",
            "Epoch 57/481\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.7013 - accuracy: 0.5192\n",
            "Epoch 58/481\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.6864 - accuracy: 0.5577\n",
            "Epoch 59/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6848 - accuracy: 0.5962\n",
            "Epoch 60/481\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.7174 - accuracy: 0.5192\n",
            "Epoch 61/481\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.6737 - accuracy: 0.5385\n",
            "Epoch 62/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7129 - accuracy: 0.5577\n",
            "Epoch 63/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6832 - accuracy: 0.5577\n",
            "Epoch 64/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6487 - accuracy: 0.6154\n",
            "Epoch 65/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6967 - accuracy: 0.5385\n",
            "Epoch 66/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7412 - accuracy: 0.5192\n",
            "Epoch 67/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6987 - accuracy: 0.5577\n",
            "Epoch 68/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6491 - accuracy: 0.6731\n",
            "Epoch 69/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6899 - accuracy: 0.5385\n",
            "Epoch 70/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6699 - accuracy: 0.5577\n",
            "Epoch 71/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6703 - accuracy: 0.5769\n",
            "Epoch 72/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6601 - accuracy: 0.6154\n",
            "Epoch 73/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6286 - accuracy: 0.7115\n",
            "Epoch 74/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6973 - accuracy: 0.5385\n",
            "Epoch 75/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6398 - accuracy: 0.6346\n",
            "Epoch 76/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6593 - accuracy: 0.5962\n",
            "Epoch 77/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5766 - accuracy: 0.6731\n",
            "Epoch 78/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6059 - accuracy: 0.6923\n",
            "Epoch 79/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.6292 - accuracy: 0.6538\n",
            "Epoch 80/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5395 - accuracy: 0.7308\n",
            "Epoch 81/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5626 - accuracy: 0.7500\n",
            "Epoch 82/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5503 - accuracy: 0.7500\n",
            "Epoch 83/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.5487 - accuracy: 0.7500\n",
            "Epoch 84/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5391 - accuracy: 0.6923\n",
            "Epoch 85/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4994 - accuracy: 0.8077\n",
            "Epoch 86/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5546 - accuracy: 0.6731\n",
            "Epoch 87/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5504 - accuracy: 0.6538\n",
            "Epoch 88/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5337 - accuracy: 0.7500\n",
            "Epoch 89/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5178 - accuracy: 0.6923\n",
            "Epoch 90/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.5615 - accuracy: 0.6923\n",
            "Epoch 91/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.4979 - accuracy: 0.7885\n",
            "Epoch 92/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.4940 - accuracy: 0.7115\n",
            "Epoch 93/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.4731 - accuracy: 0.7885\n",
            "Epoch 94/481\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4793 - accuracy: 0.7500\n",
            "Epoch 95/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5041 - accuracy: 0.7692\n",
            "Epoch 96/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.4410 - accuracy: 0.7885\n",
            "Epoch 97/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.4574 - accuracy: 0.7692\n",
            "Epoch 98/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.4813 - accuracy: 0.7692\n",
            "Epoch 99/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.4642 - accuracy: 0.7692\n",
            "Epoch 100/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.4564 - accuracy: 0.8077\n",
            "Epoch 101/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4593 - accuracy: 0.7500\n",
            "Epoch 102/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4378 - accuracy: 0.8269\n",
            "Epoch 103/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4452 - accuracy: 0.7885\n",
            "Epoch 104/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4385 - accuracy: 0.8269\n",
            "Epoch 105/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4486 - accuracy: 0.8077\n",
            "Epoch 106/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4457 - accuracy: 0.8077\n",
            "Epoch 107/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4226 - accuracy: 0.8654\n",
            "Epoch 108/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3915 - accuracy: 0.7885\n",
            "Epoch 109/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.4406 - accuracy: 0.7692\n",
            "Epoch 110/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.3853 - accuracy: 0.8077\n",
            "Epoch 111/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4208 - accuracy: 0.8077\n",
            "Epoch 112/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.4541 - accuracy: 0.8077\n",
            "Epoch 113/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4037 - accuracy: 0.8269\n",
            "Epoch 114/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3949 - accuracy: 0.8269\n",
            "Epoch 115/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4067 - accuracy: 0.7692\n",
            "Epoch 116/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3855 - accuracy: 0.8269\n",
            "Epoch 117/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3794 - accuracy: 0.8269\n",
            "Epoch 118/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3950 - accuracy: 0.8077\n",
            "Epoch 119/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4300 - accuracy: 0.7692\n",
            "Epoch 120/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3514 - accuracy: 0.8654\n",
            "Epoch 121/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3809 - accuracy: 0.8269\n",
            "Epoch 122/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3978 - accuracy: 0.8269\n",
            "Epoch 123/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3989 - accuracy: 0.8269\n",
            "Epoch 124/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3717 - accuracy: 0.8462\n",
            "Epoch 125/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3588 - accuracy: 0.8269\n",
            "Epoch 126/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3115 - accuracy: 0.8462\n",
            "Epoch 127/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.3847 - accuracy: 0.7885\n",
            "Epoch 128/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.3641 - accuracy: 0.8077\n",
            "Epoch 129/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4017 - accuracy: 0.7885\n",
            "Epoch 130/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3939 - accuracy: 0.7885\n",
            "Epoch 131/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3486 - accuracy: 0.8462\n",
            "Epoch 132/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3622 - accuracy: 0.8462\n",
            "Epoch 133/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3474 - accuracy: 0.8269\n",
            "Epoch 134/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3444 - accuracy: 0.8462\n",
            "Epoch 135/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3048 - accuracy: 0.8846\n",
            "Epoch 136/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3404 - accuracy: 0.8077\n",
            "Epoch 137/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3373 - accuracy: 0.8077\n",
            "Epoch 138/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3483 - accuracy: 0.8269\n",
            "Epoch 139/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3461 - accuracy: 0.8077\n",
            "Epoch 140/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3102 - accuracy: 0.8654\n",
            "Epoch 141/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3302 - accuracy: 0.8269\n",
            "Epoch 142/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2790 - accuracy: 0.8846\n",
            "Epoch 143/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3335 - accuracy: 0.8269\n",
            "Epoch 144/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3356 - accuracy: 0.8269\n",
            "Epoch 145/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2937 - accuracy: 0.8846\n",
            "Epoch 146/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3100 - accuracy: 0.8269\n",
            "Epoch 147/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3010 - accuracy: 0.8846\n",
            "Epoch 148/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3111 - accuracy: 0.8654\n",
            "Epoch 149/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.3094 - accuracy: 0.8462\n",
            "Epoch 150/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.3113 - accuracy: 0.8269\n",
            "Epoch 151/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2974 - accuracy: 0.8846\n",
            "Epoch 152/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2821 - accuracy: 0.8462\n",
            "Epoch 153/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.3486 - accuracy: 0.8654\n",
            "Epoch 154/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.2759 - accuracy: 0.8462\n",
            "Epoch 155/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.3424 - accuracy: 0.8269\n",
            "Epoch 156/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2963 - accuracy: 0.8654\n",
            "Epoch 157/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2640 - accuracy: 0.8846\n",
            "Epoch 158/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3736 - accuracy: 0.7885\n",
            "Epoch 159/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2783 - accuracy: 0.8462\n",
            "Epoch 160/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.3484 - accuracy: 0.8077\n",
            "Epoch 161/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3136 - accuracy: 0.8462\n",
            "Epoch 162/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.2724 - accuracy: 0.8846\n",
            "Epoch 163/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.3211 - accuracy: 0.8462\n",
            "Epoch 164/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2676 - accuracy: 0.8846\n",
            "Epoch 165/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2689 - accuracy: 0.9038\n",
            "Epoch 166/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3064 - accuracy: 0.8462\n",
            "Epoch 167/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2659 - accuracy: 0.9038\n",
            "Epoch 168/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2671 - accuracy: 0.9038\n",
            "Epoch 169/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2667 - accuracy: 0.9038\n",
            "Epoch 170/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.2708 - accuracy: 0.9038\n",
            "Epoch 171/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2578 - accuracy: 0.9231\n",
            "Epoch 172/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2663 - accuracy: 0.8846\n",
            "Epoch 173/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.2655 - accuracy: 0.8846\n",
            "Epoch 174/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2504 - accuracy: 0.8846\n",
            "Epoch 175/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2395 - accuracy: 0.9038\n",
            "Epoch 176/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2433 - accuracy: 0.9038\n",
            "Epoch 177/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2436 - accuracy: 0.9231\n",
            "Epoch 178/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.2609 - accuracy: 0.9038\n",
            "Epoch 179/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2245 - accuracy: 0.9231\n",
            "Epoch 180/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2278 - accuracy: 0.9231\n",
            "Epoch 181/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.2409 - accuracy: 0.9231\n",
            "Epoch 182/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2419 - accuracy: 0.9038\n",
            "Epoch 183/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2301 - accuracy: 0.9038\n",
            "Epoch 184/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2169 - accuracy: 0.9231\n",
            "Epoch 185/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2420 - accuracy: 0.9038\n",
            "Epoch 186/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2297 - accuracy: 0.9231\n",
            "Epoch 187/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2214 - accuracy: 0.8846\n",
            "Epoch 188/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2193 - accuracy: 0.9423\n",
            "Epoch 189/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2060 - accuracy: 0.9231\n",
            "Epoch 190/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2277 - accuracy: 0.8846\n",
            "Epoch 191/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2136 - accuracy: 0.9423\n",
            "Epoch 192/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2408 - accuracy: 0.8846\n",
            "Epoch 193/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2021 - accuracy: 0.9231\n",
            "Epoch 194/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1834 - accuracy: 0.9231\n",
            "Epoch 195/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1997 - accuracy: 0.9423\n",
            "Epoch 196/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2042 - accuracy: 0.8846\n",
            "Epoch 197/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.1996 - accuracy: 0.9231\n",
            "Epoch 198/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1937 - accuracy: 0.9423\n",
            "Epoch 199/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1823 - accuracy: 0.9231\n",
            "Epoch 200/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1876 - accuracy: 0.9231\n",
            "Epoch 201/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1852 - accuracy: 0.9038\n",
            "Epoch 202/481\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2455 - accuracy: 0.8654\n",
            "Epoch 203/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2062 - accuracy: 0.8654\n",
            "Epoch 204/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2038 - accuracy: 0.9038\n",
            "Epoch 205/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2349 - accuracy: 0.8846\n",
            "Epoch 206/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1847 - accuracy: 0.9038\n",
            "Epoch 207/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2414 - accuracy: 0.8846\n",
            "Epoch 208/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2151 - accuracy: 0.8846\n",
            "Epoch 209/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1991 - accuracy: 0.9038\n",
            "Epoch 210/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2392 - accuracy: 0.9038\n",
            "Epoch 211/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1544 - accuracy: 0.9423\n",
            "Epoch 212/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1761 - accuracy: 0.9231\n",
            "Epoch 213/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1842 - accuracy: 0.9231\n",
            "Epoch 214/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1544 - accuracy: 0.9423\n",
            "Epoch 215/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1394 - accuracy: 0.9423\n",
            "Epoch 216/481\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1504 - accuracy: 0.9231\n",
            "Epoch 217/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1575 - accuracy: 0.9038\n",
            "Epoch 218/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1612 - accuracy: 0.9231\n",
            "Epoch 219/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1416 - accuracy: 0.9423\n",
            "Epoch 220/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1494 - accuracy: 0.9231\n",
            "Epoch 221/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1265 - accuracy: 0.9423\n",
            "Epoch 222/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1480 - accuracy: 0.9423\n",
            "Epoch 223/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1318 - accuracy: 0.9231\n",
            "Epoch 224/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1650 - accuracy: 0.8846\n",
            "Epoch 225/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1222 - accuracy: 0.9423\n",
            "Epoch 226/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1845 - accuracy: 0.9231\n",
            "Epoch 227/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1400 - accuracy: 0.9231\n",
            "Epoch 228/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1292 - accuracy: 0.9231\n",
            "Epoch 229/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1422 - accuracy: 0.9423\n",
            "Epoch 230/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1147 - accuracy: 0.9038\n",
            "Epoch 231/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1286 - accuracy: 0.9231\n",
            "Epoch 232/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1276 - accuracy: 0.9231\n",
            "Epoch 233/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1152 - accuracy: 0.9615\n",
            "Epoch 234/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1199 - accuracy: 0.9615\n",
            "Epoch 235/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1164 - accuracy: 0.9231\n",
            "Epoch 236/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1022 - accuracy: 0.9423\n",
            "Epoch 237/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1055 - accuracy: 0.9423\n",
            "Epoch 238/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1030 - accuracy: 0.9615\n",
            "Epoch 239/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0925 - accuracy: 0.9423\n",
            "Epoch 240/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1167 - accuracy: 0.9231\n",
            "Epoch 241/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.1128 - accuracy: 0.9615\n",
            "Epoch 242/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1043 - accuracy: 0.9231\n",
            "Epoch 243/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0895 - accuracy: 0.9423\n",
            "Epoch 244/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0954 - accuracy: 0.9423\n",
            "Epoch 245/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0806 - accuracy: 0.9615\n",
            "Epoch 246/481\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1174 - accuracy: 0.9231\n",
            "Epoch 247/481\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0896 - accuracy: 0.9615\n",
            "Epoch 248/481\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.1263 - accuracy: 0.9231\n",
            "Epoch 249/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0918 - accuracy: 0.9615\n",
            "Epoch 250/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.1039 - accuracy: 0.9231\n",
            "Epoch 251/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0885 - accuracy: 0.9615\n",
            "Epoch 252/481\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0837 - accuracy: 0.9615\n",
            "Epoch 253/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0978 - accuracy: 0.9231\n",
            "Epoch 254/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0795 - accuracy: 0.9423\n",
            "Epoch 255/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1032 - accuracy: 0.9231\n",
            "Epoch 256/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0923 - accuracy: 0.9615\n",
            "Epoch 257/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0827 - accuracy: 0.9615\n",
            "Epoch 258/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1064 - accuracy: 0.9423\n",
            "Epoch 259/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1029 - accuracy: 0.9231\n",
            "Epoch 260/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1802 - accuracy: 0.9038\n",
            "Epoch 261/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0982 - accuracy: 0.9615\n",
            "Epoch 262/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0884 - accuracy: 0.9615\n",
            "Epoch 263/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1402 - accuracy: 0.9615\n",
            "Epoch 264/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0872 - accuracy: 0.9615\n",
            "Epoch 265/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0982 - accuracy: 0.9423\n",
            "Epoch 266/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0896 - accuracy: 0.9615\n",
            "Epoch 267/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0880 - accuracy: 0.9423\n",
            "Epoch 268/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0818 - accuracy: 0.9615\n",
            "Epoch 269/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0850 - accuracy: 0.9808\n",
            "Epoch 270/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0707 - accuracy: 0.9615\n",
            "Epoch 271/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0756 - accuracy: 0.9808\n",
            "Epoch 272/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0762 - accuracy: 0.9808\n",
            "Epoch 273/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0774 - accuracy: 0.9808\n",
            "Epoch 274/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0616 - accuracy: 0.9808\n",
            "Epoch 275/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0738 - accuracy: 0.9808\n",
            "Epoch 276/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0910 - accuracy: 0.9615\n",
            "Epoch 277/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0714 - accuracy: 0.9808\n",
            "Epoch 278/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0712 - accuracy: 0.9808\n",
            "Epoch 279/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0745 - accuracy: 0.9808\n",
            "Epoch 280/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0697 - accuracy: 0.9808\n",
            "Epoch 281/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0686 - accuracy: 0.9423\n",
            "Epoch 282/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0706 - accuracy: 0.9615\n",
            "Epoch 283/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0606 - accuracy: 0.9808\n",
            "Epoch 284/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0616 - accuracy: 0.9808\n",
            "Epoch 285/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0707 - accuracy: 0.9423\n",
            "Epoch 286/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0616 - accuracy: 0.9808\n",
            "Epoch 287/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0596 - accuracy: 0.9808\n",
            "Epoch 288/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0630 - accuracy: 1.0000\n",
            "Epoch 289/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0586 - accuracy: 0.9808\n",
            "Epoch 290/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0586 - accuracy: 0.9808\n",
            "Epoch 291/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0610 - accuracy: 0.9808\n",
            "Epoch 292/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0682 - accuracy: 0.9808\n",
            "Epoch 293/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0567 - accuracy: 0.9808\n",
            "Epoch 294/481\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0546 - accuracy: 1.0000\n",
            "Epoch 295/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0601 - accuracy: 0.9808\n",
            "Epoch 296/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0657 - accuracy: 0.9808\n",
            "Epoch 297/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0532 - accuracy: 0.9808\n",
            "Epoch 298/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0678 - accuracy: 0.9615\n",
            "Epoch 299/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0750 - accuracy: 0.9615\n",
            "Epoch 300/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0605 - accuracy: 0.9808\n",
            "Epoch 301/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0667 - accuracy: 0.9808\n",
            "Epoch 302/481\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0554 - accuracy: 0.9808\n",
            "Epoch 303/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0952 - accuracy: 0.9423\n",
            "Epoch 304/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0777 - accuracy: 0.9615\n",
            "Epoch 305/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0670 - accuracy: 0.9808\n",
            "Epoch 306/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0489 - accuracy: 0.9808\n",
            "Epoch 307/481\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0927 - accuracy: 0.9423\n",
            "Epoch 308/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0666 - accuracy: 0.9615\n",
            "Epoch 309/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0721 - accuracy: 0.9615\n",
            "Epoch 310/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0509 - accuracy: 0.9808\n",
            "Epoch 311/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0893 - accuracy: 0.9615\n",
            "Epoch 312/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0540 - accuracy: 0.9808\n",
            "Epoch 313/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0792 - accuracy: 0.9231\n",
            "Epoch 314/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0550 - accuracy: 0.9808\n",
            "Epoch 315/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0548 - accuracy: 0.9808\n",
            "Epoch 316/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0470 - accuracy: 0.9808\n",
            "Epoch 317/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0523 - accuracy: 0.9808\n",
            "Epoch 318/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0484 - accuracy: 0.9808\n",
            "Epoch 319/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0586 - accuracy: 0.9808\n",
            "Epoch 320/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0617 - accuracy: 0.9808\n",
            "Epoch 321/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0495 - accuracy: 0.9808\n",
            "Epoch 322/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0549 - accuracy: 0.9808\n",
            "Epoch 323/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0578 - accuracy: 0.9808\n",
            "Epoch 324/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0443 - accuracy: 0.9808\n",
            "Epoch 325/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0510 - accuracy: 0.9808\n",
            "Epoch 326/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0492 - accuracy: 0.9808\n",
            "Epoch 327/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0524 - accuracy: 0.9808\n",
            "Epoch 328/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0454 - accuracy: 0.9808\n",
            "Epoch 329/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0497 - accuracy: 0.9808\n",
            "Epoch 330/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0540 - accuracy: 0.9808\n",
            "Epoch 331/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0504 - accuracy: 0.9808\n",
            "Epoch 332/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0401 - accuracy: 0.9808\n",
            "Epoch 333/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0479 - accuracy: 0.9808\n",
            "Epoch 334/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0501 - accuracy: 0.9808\n",
            "Epoch 335/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0506 - accuracy: 0.9808\n",
            "Epoch 336/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0553 - accuracy: 0.9808\n",
            "Epoch 337/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0526 - accuracy: 0.9808\n",
            "Epoch 338/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0407 - accuracy: 0.9808\n",
            "Epoch 339/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0430 - accuracy: 0.9808\n",
            "Epoch 340/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0505 - accuracy: 0.9808\n",
            "Epoch 341/481\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0474 - accuracy: 0.9808\n",
            "Epoch 342/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0476 - accuracy: 0.9808\n",
            "Epoch 343/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0494 - accuracy: 0.9615\n",
            "Epoch 344/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0492 - accuracy: 0.9808\n",
            "Epoch 345/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0500 - accuracy: 0.9808\n",
            "Epoch 346/481\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0542 - accuracy: 0.9808\n",
            "Epoch 347/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0486 - accuracy: 0.9808\n",
            "Epoch 348/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0391 - accuracy: 0.9808\n",
            "Epoch 349/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0502 - accuracy: 0.9808\n",
            "Epoch 350/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0390 - accuracy: 0.9808\n",
            "Epoch 351/481\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0544 - accuracy: 0.9808\n",
            "Epoch 352/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0637 - accuracy: 0.9615\n",
            "Epoch 353/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0432 - accuracy: 0.9615\n",
            "Epoch 354/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1489 - accuracy: 0.9423\n",
            "Epoch 355/481\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5375 - accuracy: 0.8654\n",
            "Epoch 356/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4650 - accuracy: 0.8654\n",
            "Epoch 357/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8046 - accuracy: 0.8269\n",
            "Epoch 358/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.8596 - accuracy: 0.7115\n",
            "Epoch 359/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.4417 - accuracy: 0.7885\n",
            "Epoch 360/481\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.3451 - accuracy: 0.9038\n",
            "Epoch 361/481\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.9169 - accuracy: 0.7692\n",
            "Epoch 362/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6916 - accuracy: 0.8269\n",
            "Epoch 363/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2933 - accuracy: 0.9231\n",
            "Epoch 364/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.6029 - accuracy: 0.8654\n",
            "Epoch 365/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.3867 - accuracy: 0.9038\n",
            "Epoch 366/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1322 - accuracy: 0.9615\n",
            "Epoch 367/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1913 - accuracy: 0.9423\n",
            "Epoch 368/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1482 - accuracy: 0.9423\n",
            "Epoch 369/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1060 - accuracy: 0.9615\n",
            "Epoch 370/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1183 - accuracy: 0.9423\n",
            "Epoch 371/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1264 - accuracy: 0.9423\n",
            "Epoch 372/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0986 - accuracy: 0.9423\n",
            "Epoch 373/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1018 - accuracy: 0.9808\n",
            "Epoch 374/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9808\n",
            "Epoch 375/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0972 - accuracy: 0.9615\n",
            "Epoch 376/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0868 - accuracy: 0.9423\n",
            "Epoch 377/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0855 - accuracy: 0.9808\n",
            "Epoch 378/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0888 - accuracy: 0.9808\n",
            "Epoch 379/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0756 - accuracy: 0.9615\n",
            "Epoch 380/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0821 - accuracy: 0.9615\n",
            "Epoch 381/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0752 - accuracy: 0.9808\n",
            "Epoch 382/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0761 - accuracy: 0.9808\n",
            "Epoch 383/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0727 - accuracy: 0.9808\n",
            "Epoch 384/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0770 - accuracy: 0.9808\n",
            "Epoch 385/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0768 - accuracy: 0.9808\n",
            "Epoch 386/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0728 - accuracy: 0.9808\n",
            "Epoch 387/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0699 - accuracy: 0.9808\n",
            "Epoch 388/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0695 - accuracy: 0.9808\n",
            "Epoch 389/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0721 - accuracy: 0.9808\n",
            "Epoch 390/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0571 - accuracy: 0.9808\n",
            "Epoch 391/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0720 - accuracy: 0.9615\n",
            "Epoch 392/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0638 - accuracy: 0.9808\n",
            "Epoch 393/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0623 - accuracy: 0.9615\n",
            "Epoch 394/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0651 - accuracy: 0.9808\n",
            "Epoch 395/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0635 - accuracy: 0.9808\n",
            "Epoch 396/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0552 - accuracy: 0.9808\n",
            "Epoch 397/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0663 - accuracy: 0.9615\n",
            "Epoch 398/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0699 - accuracy: 0.9808\n",
            "Epoch 399/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0590 - accuracy: 0.9808\n",
            "Epoch 400/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0601 - accuracy: 1.0000\n",
            "Epoch 401/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0593 - accuracy: 0.9808\n",
            "Epoch 402/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0644 - accuracy: 0.9615\n",
            "Epoch 403/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0653 - accuracy: 0.9808\n",
            "Epoch 404/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0572 - accuracy: 0.9808\n",
            "Epoch 405/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0610 - accuracy: 0.9808\n",
            "Epoch 406/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0595 - accuracy: 0.9808\n",
            "Epoch 407/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0605 - accuracy: 0.9808\n",
            "Epoch 408/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0584 - accuracy: 0.9615\n",
            "Epoch 409/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0561 - accuracy: 0.9808\n",
            "Epoch 410/481\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0554 - accuracy: 0.9808\n",
            "Epoch 411/481\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0533 - accuracy: 0.9808\n",
            "Epoch 412/481\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0590 - accuracy: 0.9808\n",
            "Epoch 413/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0612 - accuracy: 0.9808\n",
            "Epoch 414/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0573 - accuracy: 0.9808\n",
            "Epoch 415/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0679 - accuracy: 0.9615\n",
            "Epoch 416/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0508 - accuracy: 0.9808\n",
            "Epoch 417/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0493 - accuracy: 0.9808\n",
            "Epoch 418/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0533 - accuracy: 0.9808\n",
            "Epoch 419/481\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0641 - accuracy: 0.9615\n",
            "Epoch 420/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0532 - accuracy: 0.9808\n",
            "Epoch 421/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0558 - accuracy: 0.9808\n",
            "Epoch 422/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0500 - accuracy: 0.9808\n",
            "Epoch 423/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0479 - accuracy: 0.9808\n",
            "Epoch 424/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0524 - accuracy: 0.9808\n",
            "Epoch 425/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0572 - accuracy: 0.9808\n",
            "Epoch 426/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0518 - accuracy: 0.9808\n",
            "Epoch 427/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0455 - accuracy: 1.0000\n",
            "Epoch 428/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0487 - accuracy: 0.9808\n",
            "Epoch 429/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0474 - accuracy: 0.9808\n",
            "Epoch 430/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0511 - accuracy: 0.9808\n",
            "Epoch 431/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0539 - accuracy: 0.9808\n",
            "Epoch 432/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0515 - accuracy: 0.9808\n",
            "Epoch 433/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0560 - accuracy: 0.9808\n",
            "Epoch 434/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0507 - accuracy: 0.9808\n",
            "Epoch 435/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0514 - accuracy: 0.9808\n",
            "Epoch 436/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0562 - accuracy: 0.9808\n",
            "Epoch 437/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0501 - accuracy: 0.9808\n",
            "Epoch 438/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0495 - accuracy: 0.9808\n",
            "Epoch 439/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0532 - accuracy: 0.9808\n",
            "Epoch 440/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0558 - accuracy: 0.9808\n",
            "Epoch 441/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0522 - accuracy: 0.9808\n",
            "Epoch 442/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0554 - accuracy: 0.9808\n",
            "Epoch 443/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0517 - accuracy: 0.9808\n",
            "Epoch 444/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0489 - accuracy: 0.9808\n",
            "Epoch 445/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0499 - accuracy: 0.9808\n",
            "Epoch 446/481\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0437 - accuracy: 0.9808\n",
            "Epoch 447/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0491 - accuracy: 0.9808\n",
            "Epoch 448/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0569 - accuracy: 0.9808\n",
            "Epoch 449/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0489 - accuracy: 0.9808\n",
            "Epoch 450/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0505 - accuracy: 0.9808\n",
            "Epoch 451/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0398 - accuracy: 0.9808\n",
            "Epoch 452/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0475 - accuracy: 0.9808\n",
            "Epoch 453/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0501 - accuracy: 0.9808\n",
            "Epoch 454/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0455 - accuracy: 0.9808\n",
            "Epoch 455/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0457 - accuracy: 0.9808\n",
            "Epoch 456/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0400 - accuracy: 0.9808\n",
            "Epoch 457/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0533 - accuracy: 0.9808\n",
            "Epoch 458/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0514 - accuracy: 0.9808\n",
            "Epoch 459/481\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0544 - accuracy: 0.9808\n",
            "Epoch 460/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0519 - accuracy: 0.9808\n",
            "Epoch 461/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0432 - accuracy: 0.9808\n",
            "Epoch 462/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0492 - accuracy: 0.9808\n",
            "Epoch 463/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0475 - accuracy: 0.9808\n",
            "Epoch 464/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0507 - accuracy: 0.9808\n",
            "Epoch 465/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0501 - accuracy: 0.9808\n",
            "Epoch 466/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0508 - accuracy: 0.9808\n",
            "Epoch 467/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0565 - accuracy: 0.9808\n",
            "Epoch 468/481\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0538 - accuracy: 0.9808\n",
            "Epoch 469/481\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0442 - accuracy: 1.0000\n",
            "Epoch 470/481\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0450 - accuracy: 0.9808\n",
            "Epoch 471/481\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0429 - accuracy: 0.9808\n",
            "Epoch 472/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0493 - accuracy: 0.9808\n",
            "Epoch 473/481\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0473 - accuracy: 0.9808\n",
            "Epoch 474/481\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.0446 - accuracy: 0.9808\n",
            "Epoch 475/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0464 - accuracy: 0.9808\n",
            "Epoch 476/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0468 - accuracy: 0.9808\n",
            "Epoch 477/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0477 - accuracy: 0.9808\n",
            "Epoch 478/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0516 - accuracy: 0.9808\n",
            "Epoch 479/481\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0511 - accuracy: 0.9808\n",
            "Epoch 480/481\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0460 - accuracy: 0.9808\n",
            "Epoch 481/481\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0443 - accuracy: 0.9808\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d1b9b4279d0>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7TmZPBf12zl",
        "outputId": "096dc1e1-ce4b-41bf-b824-19cb79a09855"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30,)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "arr2_vec.shape[2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyOs8niJ2GWL",
        "outputId": "7c03c244-e415-4c6b-d94b-65cc0134df8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what has', 'has NN', 'NN VBN', 'VBN TO', 'TO NN', 'what has NN', 'has NN VBN', 'NN VBN TO', 'VBN TO NN', 'what has NN VBN', 'has NN VBN TO', 'NN VBN TO NN', 'who has', 'has VBN', 'VBN NN', 'NN TO', 'who has VBN', 'has VBN NN', 'VBN NN TO', 'NN TO NN', 'who has VBN NN', 'has VBN NN TO', 'VBN NN TO NN', 'has been', 'been VBN', 'what has been', 'has been VBN', 'been VBN TO', 'what has been VBN', 'has been VBN TO', 'been VBN TO NN', 'did NN', 'NN VB', 'VB NN', 'NN Time', 'did NN VB', 'NN VB NN', 'VB NN Time', 'did NN VB NN', 'NN VB NN Time', 'from whom', 'whom do', 'do NN', 'from whom do', 'whom do NN', 'do NN VB', 'from whom do NN', 'whom do NN VB', 'do NN VB NN', 'TO whom', 'whom has', 'TO whom has', 'whom has NN', 'NN VBN NN', 'TO whom has NN', 'whom has NN VBN', 'has NN VBN NN', 'NN has', 'NN has VBN', 'NN has VBN NN', 'NN by', 'by NN', 'VBN NN by', 'NN by NN', 'has VBN NN by', 'VBN NN by NN', 'VBN DT', 'DT NN', 'has VBN DT', 'VBN DT NN', 'NN has VBN DT', 'has VBN DT NN', 'NN from', 'from NN', 'VBN NN from', 'NN from NN', 'has VBN NN from', 'VBN NN from NN', 'VBN RB', 'RB RB', 'RB in', 'in DT', 'DT event', 'NN VBN RB', 'VBN RB RB', 'RB RB in', 'RB in DT', 'in DT event', 'NN VBN RB RB', 'VBN RB RB in', 'RB RB in DT', 'RB in DT event', 'who VBN', 'who VBN NN', 'who VBN NN by', 'NN VBZ', 'VBZ RB', 'NN VBZ RB', 'VBZ RB RB', 'NN VBZ RB RB', 'VBZ NN', 'NN of', 'of NN', 'NN in', 'DT place', 'NN VBZ NN', 'VBZ NN of', 'NN of NN', 'of NN in', 'NN in DT', 'in DT place', 'NN VBZ NN of', 'VBZ NN of NN', 'NN of NN in', 'of NN in DT', 'NN in DT place', 'why did', 'NN NN', 'why did NN', 'did NN NN', 'NN NN NN', 'why did NN NN', 'did NN NN NN', 'where DT', 'VBN VBN', 'where DT NN', 'DT NN VBN', 'NN VBN VBN', 'where DT NN VBN', 'DT NN VBN VBN', 'who VBN DT', 'who VBN DT NN', 'who did', 'did RB', 'RB VBN', 'VBN in', 'who did RB', 'did RB VBN', 'RB VBN in', 'VBN in DT', 'who did RB VBN', 'did RB VBN in', 'RB VBN in DT', 'VBN in DT event', 'why NN', 'VBN IN', 'IN NN', 'why NN VBN', 'NN VBN IN', 'VBN IN NN', 'why NN VBN IN', 'NN VBN IN NN', 'where NN', 'where NN VBN', 'NN VBN DT', 'where NN VBN DT', 'NN VBN DT NN', 'been VBN in', 'in DT NN', 'has been VBN in', 'been VBN in DT', 'VBN in DT NN', 'why has', 'why has NN', 'why has NN VBN', 'NN VBN NN TO', 'at what', 'what NN', 'at what NN', 'what NN NN', 'NN NN VBN', 'at what NN NN', 'what NN NN VBN', 'NN NN VBN NN', 'when has', 'when has NN', 'when has NN VBN', 'when did', 'when did NN', 'when did NN VB', 'whom did', 'from whom did', 'whom did NN', 'from whom did NN', 'whom did NN VB', 'from where', 'where did', 'from where did', 'where did NN', 'from where did NN', 'where did NN VB', 'NN did', 'why NN did', 'NN did RB', 'RB VBN IN', 'why NN did RB', 'NN did RB VBN', 'did RB VBN IN', 'RB VBN IN NN', 'TO VB', 'VBN TO VB', 'been VBN TO VB', 'why did NN VB', 'have NN', 'have NN VBN', 'have NN VBN NN', 'WP VBN', 'WP VBN NN', 'what did', 'VB TO', 'what did NN', 'NN VB TO', 'VB TO NN', 'what did NN VB', 'did NN VB TO', 'NN VB TO NN', 'when have', 'when have NN', 'when have NN VBN', 'did VB', 'what did VB', 'did VB TO', 'what did VB TO', 'did VB TO NN', 'when NN', 'when NN VBZ', 'NN VBP', 'VBP TO', 'did NN VBP', 'NN VBP TO', 'VBP TO NN', 'what did NN VBP', 'did NN VBP TO', 'NN VBP TO NN', 'VBN by', 'been VBN by', 'VBN by NN', 'has been VBN by', 'been VBN by NN', 'NN have', 'VBP in', 'in event', 'what NN have', 'NN have NN', 'have NN VBP', 'NN VBP in', 'VBP in event', 'what NN have NN', 'NN have NN VBP', 'have NN VBP in', 'NN VBP in event', 'VB at', 'at event', 'what NN did', 'NN did NN', 'NN VB at', 'VB at event', 'what NN did NN', 'NN did NN VB', 'did NN VB at', 'NN VB at event', 'VBN at', 'been VBN at', 'VBN at event', 'has been VBN at', 'been VBN at event', 'when did NN NN']\n"
          ]
        }
      ],
      "source": [
        "y=len(sentence)\n",
        "gram3=[]\n",
        "for i in range(y):\n",
        "  words = arr2[i].split()\n",
        "  bi_grams = nltk.ngrams(words, 2)\n",
        "  for gram in bi_grams:\n",
        "    gram2=' '.join(gram)\n",
        "    if not gram2 in gram3:\n",
        "      gram3.append(gram2)\n",
        "  bi_grams = nltk.ngrams(words, 3)\n",
        "  for gram in bi_grams:\n",
        "    gram2=' '.join(gram)\n",
        "    if not gram2 in gram3:\n",
        "      gram3.append(gram2)\n",
        "  bi_grams = nltk.ngrams(words, 4)\n",
        "  for gram in bi_grams:\n",
        "    gram2=' '.join(gram)\n",
        "    if not gram2 in gram3:\n",
        "      gram3.append(gram2)\n",
        "\n",
        "\n",
        "print(gram3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aq3BSXQLuuU",
        "outputId": "c5a3a9d1-9b71-4bc9-dbd0-bf9dd5e7a84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(58, 267)\n"
          ]
        }
      ],
      "source": [
        "x=len(gram3)\n",
        "y=len(sentence)\n",
        "sentence_vec=numpy.zeros((y,x))\n",
        "for i in range(y):\n",
        "  words = arr2[i].split()\n",
        "  bi_grams = nltk.ngrams(words, 2)\n",
        "  for gram in bi_grams:\n",
        "    gram2=' '.join(gram)\n",
        "    if gram2 in gram3:\n",
        "      index= gram3.index(gram2)\n",
        "      sentence_vec[i,index]=1\n",
        "  bi_grams = nltk.ngrams(words, 3)\n",
        "  for gram in bi_grams:\n",
        "    gram2=' '.join(gram)\n",
        "    if gram2 in gram3:\n",
        "      index= gram3.index(gram2)\n",
        "      sentence_vec[i,index]=1\n",
        "  bi_grams = nltk.ngrams(words, 4)\n",
        "  for gram in bi_grams:\n",
        "    gram2=' '.join(gram)\n",
        "    if gram2 in gram3:\n",
        "      index=gram3.index(gram2)\n",
        "      sentence_vec[i,index]=1\n",
        "\n",
        "\n",
        "print(sentence_vec)\n",
        "print(np.shape(sentence_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3ncTPWOPMfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f96746-38e3-45f9-e427-2e79620a15fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "total_class=8\n",
        "y=len(sentence)\n",
        "class_label=numpy.empty((total_class,y))\n",
        "output_mat=[[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0],\n",
        "            [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "            [1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "            [0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "            [0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0],\n",
        "            [0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "            [0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0],\n",
        "            [0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "            [0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "            [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "            [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "            [1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "            [1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0],\n",
        "            [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "            [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "            ]\n",
        "import numpy as np\n",
        "output_mat=np.array(output_mat).astype (float)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "output_mat.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_mat[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeJ-4TAZh4s6",
        "outputId": "64734a36-c777-4a67-ce41-0eb6bd3f6eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "TZbWARPFng2Q",
        "outputId": "9981378c-5783-4107-9295-9fcdc4ed0f95"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1ba0f26f81b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m history=model.fit(indata,output_mat,epochs=110\n\u001b[0m\u001b[1;32m     11\u001b[0m                   )\n\u001b[1;32m     12\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'indata' is not defined"
          ]
        }
      ],
      "source": [
        "sentence_vec=numpy.array(sentence_vec)\n",
        "output_mat=numpy.array(output_mat)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Dense(38,activation='relu',input_dim=8))\n",
        "model.add(Dense(16,activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history=model.fit(indata,output_mat,epochs=110\n",
        "                  )\n",
        "score=model.evaluate(indata,output_mat)\n",
        "print(score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-sYTPRvHnU9",
        "outputId": "9f2798ff-56aa-4050-aa8b-9556cc81dd8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4.796753883361816, 0.18965516984462738]\n"
          ]
        }
      ],
      "source": [
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "5UuhzEAn7faZ",
        "outputId": "b3bc1ae8-3a6d-4b8a-fd80-4212ebc3bbf7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('loss')\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw7eH1CS4DXH",
        "outputId": "dbc6def0-8c74-4e40-ac94-cdd777121e56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "ypred=model.predict(arr2_vec)\n",
        "ypredbin= numpy.empty((33, 16), dtype=int)\n",
        "for i in range(0,33):\n",
        "  for j in range(0,16):\n",
        "    if ypred[i,j]>0.86:\n",
        "      ypredbin[i,j]=1\n",
        "    else:\n",
        "      ypredbin[i,j]=0\n",
        "ypredbin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBnkGVIxFIij",
        "outputId": "f78f3ccf-d575-418c-9765-d63d6d9dc180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.16098484848484848\n",
            "0.2773246329526917\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "s=0\n",
        "s1=0\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "for i in range(33):\n",
        "  s=s+recall_score(output_mat[i,:], ypredbin[i,:])\n",
        "  s1=s1+precision_score(output_mat[i,:], ypredbin[i,:])\n",
        "\n",
        "\n",
        "s=s/33\n",
        "s\n",
        "print(s)\n",
        "s1=s1/33\n",
        "print(s1)\n",
        "s3= 2*((s1*s)/(s1+s))\n",
        "print(s3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l_QaOhQeVaO",
        "outputId": "db6eb979-d91d-4696-a5b4-ee8074f32578"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "execution_count": 78,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ypredbin.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiyFXRMrsnPX",
        "outputId": "5b55054f-6ae0-4e34-ab93-db87bbe6e600"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 43,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(gram3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ENy1tmCQByR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHCRzXNW6SMt",
        "outputId": "347be005-56aa-4ffd-b4c9-fb45895da9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "4ujBdvF6R-oi",
        "outputId": "67ce0bb6-1aa3-4983-e6d4-4f1d2b482983"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0b0c642234d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-zA-Z0-9\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'arr1' is not defined"
          ]
        }
      ],
      "source": [
        "  import re\n",
        "  s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', arr1[0][:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjnCEyD96w3j",
        "outputId": "d8257e18-e883-40f2-9d01-1d2d2831a1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "-2qtJQONEGOG",
        "outputId": "3a98ffa0-684e-44ff-aa23-58c21d86e292"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8116cf47107a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "A=model.predict(sentence_vec)\n",
        "X=metrics.recall_score(A,output_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6h5DPq8GRw1",
        "outputId": "f45bc2fc-4c10-401d-ced3-1fe5ed99686b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True]])"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A>0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-PvcSngUq-2",
        "outputId": "ac089188-17a2-4a56-f7cd-bc0e7b259c79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=5, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf= DecisionTreeClassifier()\n",
        "clf=DecisionTreeClassifier(ccp_alpha=t)\n",
        "clf.fit(sentence_vec,output_mat[:,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "4oDpEO8DVnPH",
        "outputId": "845bf533-2796-4a4d-ffbb-2fc121784627"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Text(167.4, 108.72, 'gini = 0.5\\nsamples = 33\\nvalue = [16, 17]')]"
            ]
          },
          "execution_count": 56,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn import tree\n",
        "tree.plot_tree(clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rJ8ys8DPXcYy",
        "outputId": "62cd4a87-2af2-4d57-a6b4-bf4f435067eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-729420bc-6d8c-4ad4-aed7-731311a336ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>principal component 1</th>\n",
              "      <th>principal component 2</th>\n",
              "      <th>principal component 3</th>\n",
              "      <th>principal component 4</th>\n",
              "      <th>principal component 5</th>\n",
              "      <th>principal component 6</th>\n",
              "      <th>principal component 7</th>\n",
              "      <th>principal component 8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.160352</td>\n",
              "      <td>0.397215</td>\n",
              "      <td>-1.623879</td>\n",
              "      <td>-0.515795</td>\n",
              "      <td>0.679641</td>\n",
              "      <td>1.913441</td>\n",
              "      <td>0.319530</td>\n",
              "      <td>0.604376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.945537</td>\n",
              "      <td>-0.771097</td>\n",
              "      <td>0.242077</td>\n",
              "      <td>1.399677</td>\n",
              "      <td>-0.958071</td>\n",
              "      <td>-0.559076</td>\n",
              "      <td>0.495541</td>\n",
              "      <td>1.129940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.677822</td>\n",
              "      <td>2.633074</td>\n",
              "      <td>-0.848751</td>\n",
              "      <td>0.454017</td>\n",
              "      <td>-0.527108</td>\n",
              "      <td>-0.018329</td>\n",
              "      <td>0.039616</td>\n",
              "      <td>0.137539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.852267</td>\n",
              "      <td>-0.309926</td>\n",
              "      <td>-0.454957</td>\n",
              "      <td>-0.384392</td>\n",
              "      <td>-0.565030</td>\n",
              "      <td>0.009492</td>\n",
              "      <td>0.066215</td>\n",
              "      <td>0.114362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.105451</td>\n",
              "      <td>-0.149838</td>\n",
              "      <td>-0.004318</td>\n",
              "      <td>-0.777841</td>\n",
              "      <td>-0.567400</td>\n",
              "      <td>0.039094</td>\n",
              "      <td>0.204943</td>\n",
              "      <td>0.238442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-1.295429</td>\n",
              "      <td>-1.017867</td>\n",
              "      <td>-0.639597</td>\n",
              "      <td>-0.718889</td>\n",
              "      <td>-0.112993</td>\n",
              "      <td>0.288842</td>\n",
              "      <td>-0.222654</td>\n",
              "      <td>-0.598112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.427147</td>\n",
              "      <td>-0.343485</td>\n",
              "      <td>1.335679</td>\n",
              "      <td>0.619483</td>\n",
              "      <td>-0.742122</td>\n",
              "      <td>0.720054</td>\n",
              "      <td>0.147259</td>\n",
              "      <td>0.421469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.515604</td>\n",
              "      <td>-0.370641</td>\n",
              "      <td>1.801747</td>\n",
              "      <td>0.869623</td>\n",
              "      <td>-1.118852</td>\n",
              "      <td>1.109565</td>\n",
              "      <td>0.130188</td>\n",
              "      <td>0.408961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.151402</td>\n",
              "      <td>0.039479</td>\n",
              "      <td>1.233488</td>\n",
              "      <td>0.174537</td>\n",
              "      <td>-0.073624</td>\n",
              "      <td>0.710858</td>\n",
              "      <td>0.022073</td>\n",
              "      <td>0.128964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.473127</td>\n",
              "      <td>-0.392202</td>\n",
              "      <td>1.607595</td>\n",
              "      <td>0.802692</td>\n",
              "      <td>-0.968553</td>\n",
              "      <td>1.003332</td>\n",
              "      <td>0.220407</td>\n",
              "      <td>0.637731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.353269</td>\n",
              "      <td>-0.042001</td>\n",
              "      <td>0.443967</td>\n",
              "      <td>-1.006799</td>\n",
              "      <td>1.162071</td>\n",
              "      <td>-0.497963</td>\n",
              "      <td>0.397164</td>\n",
              "      <td>0.484993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-0.390691</td>\n",
              "      <td>-0.244855</td>\n",
              "      <td>1.145535</td>\n",
              "      <td>0.260073</td>\n",
              "      <td>-0.461385</td>\n",
              "      <td>0.402004</td>\n",
              "      <td>-0.179612</td>\n",
              "      <td>-0.465605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-0.700193</td>\n",
              "      <td>-0.553123</td>\n",
              "      <td>0.114612</td>\n",
              "      <td>-0.268982</td>\n",
              "      <td>-0.050828</td>\n",
              "      <td>-0.109823</td>\n",
              "      <td>-0.202189</td>\n",
              "      <td>-0.408759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-0.014356</td>\n",
              "      <td>0.089565</td>\n",
              "      <td>0.511771</td>\n",
              "      <td>-0.225918</td>\n",
              "      <td>0.433463</td>\n",
              "      <td>-0.083022</td>\n",
              "      <td>0.290531</td>\n",
              "      <td>-0.169596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.029591</td>\n",
              "      <td>0.209382</td>\n",
              "      <td>0.900364</td>\n",
              "      <td>-0.639115</td>\n",
              "      <td>1.153756</td>\n",
              "      <td>-0.774803</td>\n",
              "      <td>3.483891</td>\n",
              "      <td>-0.411006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.463686</td>\n",
              "      <td>-0.046286</td>\n",
              "      <td>0.233302</td>\n",
              "      <td>0.052077</td>\n",
              "      <td>0.328270</td>\n",
              "      <td>-0.208905</td>\n",
              "      <td>-0.173805</td>\n",
              "      <td>-0.379484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-0.319797</td>\n",
              "      <td>-0.065381</td>\n",
              "      <td>0.284677</td>\n",
              "      <td>-0.524473</td>\n",
              "      <td>0.508444</td>\n",
              "      <td>0.120449</td>\n",
              "      <td>-0.218505</td>\n",
              "      <td>-0.394964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.070301</td>\n",
              "      <td>0.114785</td>\n",
              "      <td>0.743683</td>\n",
              "      <td>-0.212303</td>\n",
              "      <td>0.343826</td>\n",
              "      <td>0.177416</td>\n",
              "      <td>-0.166553</td>\n",
              "      <td>-0.416783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.061589</td>\n",
              "      <td>0.330579</td>\n",
              "      <td>0.900306</td>\n",
              "      <td>-0.826985</td>\n",
              "      <td>1.264736</td>\n",
              "      <td>-1.071901</td>\n",
              "      <td>-0.113686</td>\n",
              "      <td>1.821242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.302141</td>\n",
              "      <td>-0.114280</td>\n",
              "      <td>0.313778</td>\n",
              "      <td>-0.622612</td>\n",
              "      <td>0.750557</td>\n",
              "      <td>-0.086840</td>\n",
              "      <td>-0.803937</td>\n",
              "      <td>0.447982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>-0.327897</td>\n",
              "      <td>-0.057912</td>\n",
              "      <td>0.423559</td>\n",
              "      <td>-0.528389</td>\n",
              "      <td>0.537397</td>\n",
              "      <td>0.245109</td>\n",
              "      <td>-0.244587</td>\n",
              "      <td>-0.448796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.447726</td>\n",
              "      <td>2.345381</td>\n",
              "      <td>0.136792</td>\n",
              "      <td>-0.299653</td>\n",
              "      <td>-0.225023</td>\n",
              "      <td>-0.872708</td>\n",
              "      <td>0.073375</td>\n",
              "      <td>0.046534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>-1.855220</td>\n",
              "      <td>-1.482594</td>\n",
              "      <td>-1.503758</td>\n",
              "      <td>0.311684</td>\n",
              "      <td>-0.553425</td>\n",
              "      <td>-0.803383</td>\n",
              "      <td>0.203427</td>\n",
              "      <td>0.364675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-1.475756</td>\n",
              "      <td>-1.298937</td>\n",
              "      <td>-0.945272</td>\n",
              "      <td>0.869250</td>\n",
              "      <td>-0.603979</td>\n",
              "      <td>-1.844570</td>\n",
              "      <td>0.046015</td>\n",
              "      <td>0.293279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>-1.923019</td>\n",
              "      <td>-1.552340</td>\n",
              "      <td>-1.570795</td>\n",
              "      <td>0.234902</td>\n",
              "      <td>-0.576501</td>\n",
              "      <td>-0.784917</td>\n",
              "      <td>0.174864</td>\n",
              "      <td>0.279125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.194812</td>\n",
              "      <td>-0.166350</td>\n",
              "      <td>-0.157798</td>\n",
              "      <td>0.103269</td>\n",
              "      <td>0.100830</td>\n",
              "      <td>-0.030257</td>\n",
              "      <td>-0.121991</td>\n",
              "      <td>-0.204146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.971794</td>\n",
              "      <td>-0.335446</td>\n",
              "      <td>-0.501413</td>\n",
              "      <td>-0.479573</td>\n",
              "      <td>-0.680082</td>\n",
              "      <td>0.013922</td>\n",
              "      <td>0.099627</td>\n",
              "      <td>0.164093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2.187429</td>\n",
              "      <td>-0.386759</td>\n",
              "      <td>-0.629399</td>\n",
              "      <td>-0.626525</td>\n",
              "      <td>-0.937171</td>\n",
              "      <td>0.018726</td>\n",
              "      <td>0.164245</td>\n",
              "      <td>0.295145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>-0.012131</td>\n",
              "      <td>0.117730</td>\n",
              "      <td>0.834250</td>\n",
              "      <td>-0.665136</td>\n",
              "      <td>1.214870</td>\n",
              "      <td>-0.621537</td>\n",
              "      <td>-1.554082</td>\n",
              "      <td>1.870214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>-0.419786</td>\n",
              "      <td>-0.345775</td>\n",
              "      <td>0.967100</td>\n",
              "      <td>0.569371</td>\n",
              "      <td>-0.586144</td>\n",
              "      <td>0.202956</td>\n",
              "      <td>0.173169</td>\n",
              "      <td>0.420912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>-0.304257</td>\n",
              "      <td>-0.220303</td>\n",
              "      <td>0.697967</td>\n",
              "      <td>0.033120</td>\n",
              "      <td>-0.110273</td>\n",
              "      <td>0.045788</td>\n",
              "      <td>-0.145405</td>\n",
              "      <td>-0.403668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-0.304257</td>\n",
              "      <td>-0.220303</td>\n",
              "      <td>0.697967</td>\n",
              "      <td>0.033120</td>\n",
              "      <td>-0.110273</td>\n",
              "      <td>0.045788</td>\n",
              "      <td>-0.145405</td>\n",
              "      <td>-0.403668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>-0.304257</td>\n",
              "      <td>-0.220303</td>\n",
              "      <td>0.697967</td>\n",
              "      <td>0.033120</td>\n",
              "      <td>-0.110273</td>\n",
              "      <td>0.045788</td>\n",
              "      <td>-0.145405</td>\n",
              "      <td>-0.403668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.762261</td>\n",
              "      <td>-0.290678</td>\n",
              "      <td>-0.416480</td>\n",
              "      <td>-0.340524</td>\n",
              "      <td>-0.498983</td>\n",
              "      <td>0.008152</td>\n",
              "      <td>0.055228</td>\n",
              "      <td>0.094971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>-0.512447</td>\n",
              "      <td>2.543155</td>\n",
              "      <td>-0.388484</td>\n",
              "      <td>0.085258</td>\n",
              "      <td>-0.660030</td>\n",
              "      <td>-0.245237</td>\n",
              "      <td>-0.115032</td>\n",
              "      <td>-0.168268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.966081</td>\n",
              "      <td>-0.124282</td>\n",
              "      <td>0.010548</td>\n",
              "      <td>-0.582061</td>\n",
              "      <td>-0.408253</td>\n",
              "      <td>0.027401</td>\n",
              "      <td>0.131417</td>\n",
              "      <td>0.148309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2.187429</td>\n",
              "      <td>-0.386759</td>\n",
              "      <td>-0.629399</td>\n",
              "      <td>-0.626525</td>\n",
              "      <td>-0.937171</td>\n",
              "      <td>0.018726</td>\n",
              "      <td>0.164245</td>\n",
              "      <td>0.295145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1.868055</td>\n",
              "      <td>-0.311969</td>\n",
              "      <td>-0.440588</td>\n",
              "      <td>-0.379919</td>\n",
              "      <td>-0.536063</td>\n",
              "      <td>-0.013400</td>\n",
              "      <td>0.043163</td>\n",
              "      <td>0.062708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>-0.778403</td>\n",
              "      <td>-0.634001</td>\n",
              "      <td>0.159736</td>\n",
              "      <td>-0.360497</td>\n",
              "      <td>-0.035909</td>\n",
              "      <td>-0.185355</td>\n",
              "      <td>-0.376819</td>\n",
              "      <td>-0.783339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>-1.169533</td>\n",
              "      <td>-0.891435</td>\n",
              "      <td>-0.531413</td>\n",
              "      <td>-0.554807</td>\n",
              "      <td>-0.086577</td>\n",
              "      <td>0.207291</td>\n",
              "      <td>-0.148761</td>\n",
              "      <td>-0.395286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>-0.269627</td>\n",
              "      <td>-0.193175</td>\n",
              "      <td>0.523896</td>\n",
              "      <td>0.014990</td>\n",
              "      <td>-0.067170</td>\n",
              "      <td>-0.014706</td>\n",
              "      <td>-0.076533</td>\n",
              "      <td>-0.214882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1.930537</td>\n",
              "      <td>-0.323331</td>\n",
              "      <td>-0.455823</td>\n",
              "      <td>-0.365750</td>\n",
              "      <td>-0.521821</td>\n",
              "      <td>-0.018802</td>\n",
              "      <td>0.016273</td>\n",
              "      <td>0.016386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1.341519</td>\n",
              "      <td>-0.188895</td>\n",
              "      <td>-0.867249</td>\n",
              "      <td>2.036935</td>\n",
              "      <td>1.266346</td>\n",
              "      <td>0.178821</td>\n",
              "      <td>-0.007307</td>\n",
              "      <td>-0.090930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>-0.818159</td>\n",
              "      <td>-0.675984</td>\n",
              "      <td>0.174493</td>\n",
              "      <td>-0.406938</td>\n",
              "      <td>-0.040662</td>\n",
              "      <td>-0.215823</td>\n",
              "      <td>-0.451787</td>\n",
              "      <td>-0.943277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.048105</td>\n",
              "      <td>0.071468</td>\n",
              "      <td>-0.121833</td>\n",
              "      <td>1.131935</td>\n",
              "      <td>0.980642</td>\n",
              "      <td>0.119369</td>\n",
              "      <td>0.085897</td>\n",
              "      <td>0.025267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>-0.677822</td>\n",
              "      <td>2.633074</td>\n",
              "      <td>-0.848751</td>\n",
              "      <td>0.454017</td>\n",
              "      <td>-0.527108</td>\n",
              "      <td>-0.018329</td>\n",
              "      <td>0.039616</td>\n",
              "      <td>0.137539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>-0.007721</td>\n",
              "      <td>0.084669</td>\n",
              "      <td>0.452640</td>\n",
              "      <td>-0.156560</td>\n",
              "      <td>0.329408</td>\n",
              "      <td>-0.044218</td>\n",
              "      <td>0.211624</td>\n",
              "      <td>-0.168080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.429429</td>\n",
              "      <td>-0.003116</td>\n",
              "      <td>-0.270247</td>\n",
              "      <td>1.267359</td>\n",
              "      <td>1.054062</td>\n",
              "      <td>0.057494</td>\n",
              "      <td>-0.056140</td>\n",
              "      <td>-0.247226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1.231440</td>\n",
              "      <td>-0.166126</td>\n",
              "      <td>-0.310038</td>\n",
              "      <td>0.827307</td>\n",
              "      <td>0.611552</td>\n",
              "      <td>0.067781</td>\n",
              "      <td>-0.104215</td>\n",
              "      <td>-0.220259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>-0.811419</td>\n",
              "      <td>0.030915</td>\n",
              "      <td>-0.753579</td>\n",
              "      <td>-0.740558</td>\n",
              "      <td>0.437737</td>\n",
              "      <td>1.012378</td>\n",
              "      <td>0.048941</td>\n",
              "      <td>0.063342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>-0.414107</td>\n",
              "      <td>2.136376</td>\n",
              "      <td>-0.005501</td>\n",
              "      <td>0.057516</td>\n",
              "      <td>-0.648683</td>\n",
              "      <td>-0.389838</td>\n",
              "      <td>-0.191708</td>\n",
              "      <td>-0.347057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>-0.038866</td>\n",
              "      <td>0.023248</td>\n",
              "      <td>0.597968</td>\n",
              "      <td>-0.103412</td>\n",
              "      <td>0.612621</td>\n",
              "      <td>-0.401609</td>\n",
              "      <td>-0.671342</td>\n",
              "      <td>-1.447716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1.235953</td>\n",
              "      <td>-0.150635</td>\n",
              "      <td>-0.163000</td>\n",
              "      <td>0.244434</td>\n",
              "      <td>0.281261</td>\n",
              "      <td>-0.373069</td>\n",
              "      <td>-0.645104</td>\n",
              "      <td>-0.329544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>-0.375673</td>\n",
              "      <td>2.147365</td>\n",
              "      <td>-0.111032</td>\n",
              "      <td>0.013881</td>\n",
              "      <td>-0.554022</td>\n",
              "      <td>-0.513411</td>\n",
              "      <td>-0.241023</td>\n",
              "      <td>-0.370395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-1.160352</td>\n",
              "      <td>0.397215</td>\n",
              "      <td>-1.623879</td>\n",
              "      <td>-0.515795</td>\n",
              "      <td>0.679641</td>\n",
              "      <td>1.913441</td>\n",
              "      <td>0.319530</td>\n",
              "      <td>0.604376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.506496</td>\n",
              "      <td>-0.054131</td>\n",
              "      <td>0.222645</td>\n",
              "      <td>0.062163</td>\n",
              "      <td>0.338425</td>\n",
              "      <td>-0.212812</td>\n",
              "      <td>-0.193552</td>\n",
              "      <td>-0.413577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>-1.327483</td>\n",
              "      <td>-1.053260</td>\n",
              "      <td>-0.725626</td>\n",
              "      <td>-0.596019</td>\n",
              "      <td>-0.174344</td>\n",
              "      <td>0.112345</td>\n",
              "      <td>-0.143568</td>\n",
              "      <td>-0.419003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>1.341519</td>\n",
              "      <td>-0.188895</td>\n",
              "      <td>-0.867249</td>\n",
              "      <td>2.036935</td>\n",
              "      <td>1.266346</td>\n",
              "      <td>0.178821</td>\n",
              "      <td>-0.007307</td>\n",
              "      <td>-0.090930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-729420bc-6d8c-4ad4-aed7-731311a336ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-729420bc-6d8c-4ad4-aed7-731311a336ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-729420bc-6d8c-4ad4-aed7-731311a336ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    principal component 1  ...  principal component 8\n",
              "0               -1.160352  ...               0.604376\n",
              "1               -0.945537  ...               1.129940\n",
              "2               -0.677822  ...               0.137539\n",
              "3                1.852267  ...               0.114362\n",
              "4                1.105451  ...               0.238442\n",
              "5               -1.295429  ...              -0.598112\n",
              "6               -0.427147  ...               0.421469\n",
              "7               -0.515604  ...               0.408961\n",
              "8               -0.151402  ...               0.128964\n",
              "9               -0.473127  ...               0.637731\n",
              "10              -0.353269  ...               0.484993\n",
              "11              -0.390691  ...              -0.465605\n",
              "12              -0.700193  ...              -0.408759\n",
              "13              -0.014356  ...              -0.169596\n",
              "14              -0.029591  ...              -0.411006\n",
              "15               0.463686  ...              -0.379484\n",
              "16              -0.319797  ...              -0.394964\n",
              "17              -0.070301  ...              -0.416783\n",
              "18              -0.061589  ...               1.821242\n",
              "19              -0.302141  ...               0.447982\n",
              "20              -0.327897  ...              -0.448796\n",
              "21              -0.447726  ...               0.046534\n",
              "22              -1.855220  ...               0.364675\n",
              "23              -1.475756  ...               0.293279\n",
              "24              -1.923019  ...               0.279125\n",
              "25               1.194812  ...              -0.204146\n",
              "26               1.971794  ...               0.164093\n",
              "27               2.187429  ...               0.295145\n",
              "28              -0.012131  ...               1.870214\n",
              "29              -0.419786  ...               0.420912\n",
              "30              -0.304257  ...              -0.403668\n",
              "31              -0.304257  ...              -0.403668\n",
              "32              -0.304257  ...              -0.403668\n",
              "33               1.762261  ...               0.094971\n",
              "34              -0.512447  ...              -0.168268\n",
              "35               0.966081  ...               0.148309\n",
              "36               2.187429  ...               0.295145\n",
              "37               1.868055  ...               0.062708\n",
              "38              -0.778403  ...              -0.783339\n",
              "39              -1.169533  ...              -0.395286\n",
              "40              -0.269627  ...              -0.214882\n",
              "41               1.930537  ...               0.016386\n",
              "42               1.341519  ...              -0.090930\n",
              "43              -0.818159  ...              -0.943277\n",
              "44               0.048105  ...               0.025267\n",
              "45              -0.677822  ...               0.137539\n",
              "46              -0.007721  ...              -0.168080\n",
              "47               0.429429  ...              -0.247226\n",
              "48               1.231440  ...              -0.220259\n",
              "49              -0.811419  ...               0.063342\n",
              "50              -0.414107  ...              -0.347057\n",
              "51              -0.038866  ...              -1.447716\n",
              "52               1.235953  ...              -0.329544\n",
              "53              -0.375673  ...              -0.370395\n",
              "54              -1.160352  ...               0.604376\n",
              "55               0.506496  ...              -0.413577\n",
              "56              -1.327483  ...              -0.419003\n",
              "57               1.341519  ...              -0.090930\n",
              "\n",
              "[58 rows x 8 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=8)\n",
        "principalComponents = pca.fit_transform(sentence_vec)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2','principal component 3','principal component 4','principal component 5','principal component 6','principal component 7','principal component 8'])\n",
        "principalDf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFOadYOnYjhR",
        "outputId": "5d4c674a-f023-4506-d105-ebfa59f014ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.16035228e+00,  3.97214577e-01, -1.62387919e+00,\n",
              "        -5.15795305e-01,  6.79641200e-01,  1.91344087e+00,\n",
              "         3.19529854e-01,  6.04376407e-01],\n",
              "       [-9.45536743e-01, -7.71097469e-01,  2.42076780e-01,\n",
              "         1.39967729e+00, -9.58071410e-01, -5.59075829e-01,\n",
              "         4.95540941e-01,  1.12994028e+00],\n",
              "       [-6.77822150e-01,  2.63307355e+00, -8.48750802e-01,\n",
              "         4.54016911e-01, -5.27108185e-01, -1.83288835e-02,\n",
              "         3.96163890e-02,  1.37538964e-01],\n",
              "       [ 1.85226692e+00, -3.09925985e-01, -4.54957102e-01,\n",
              "        -3.84391784e-01, -5.65030094e-01,  9.49180450e-03,\n",
              "         6.62150935e-02,  1.14361646e-01],\n",
              "       [ 1.10545125e+00, -1.49838264e-01, -4.31760680e-03,\n",
              "        -7.77841347e-01, -5.67399502e-01,  3.90940581e-02,\n",
              "         2.04942695e-01,  2.38441875e-01],\n",
              "       [-1.29542893e+00, -1.01786725e+00, -6.39597043e-01,\n",
              "        -7.18889164e-01, -1.12992500e-01,  2.88842219e-01,\n",
              "        -2.22654424e-01, -5.98112038e-01],\n",
              "       [-4.27146854e-01, -3.43485247e-01,  1.33567932e+00,\n",
              "         6.19482663e-01, -7.42122436e-01,  7.20053787e-01,\n",
              "         1.47259398e-01,  4.21469407e-01],\n",
              "       [-5.15603839e-01, -3.70641265e-01,  1.80174663e+00,\n",
              "         8.69623045e-01, -1.11885159e+00,  1.10956529e+00,\n",
              "         1.30187553e-01,  4.08960765e-01],\n",
              "       [-1.51401712e-01,  3.94787286e-02,  1.23348786e+00,\n",
              "         1.74537279e-01, -7.36237356e-02,  7.10858121e-01,\n",
              "         2.20730969e-02,  1.28964222e-01],\n",
              "       [-4.73127478e-01, -3.92201932e-01,  1.60759476e+00,\n",
              "         8.02691913e-01, -9.68553001e-01,  1.00333187e+00,\n",
              "         2.20406733e-01,  6.37731206e-01],\n",
              "       [-3.53268902e-01, -4.20006852e-02,  4.43967047e-01,\n",
              "        -1.00679858e+00,  1.16207137e+00, -4.97963453e-01,\n",
              "         3.97163910e-01,  4.84993011e-01],\n",
              "       [-3.90690581e-01, -2.44854719e-01,  1.14553521e+00,\n",
              "         2.60072578e-01, -4.61384700e-01,  4.02004423e-01,\n",
              "        -1.79612329e-01, -4.65605350e-01],\n",
              "       [-7.00192577e-01, -5.53123282e-01,  1.14612290e-01,\n",
              "        -2.68982371e-01, -5.08284946e-02, -1.09823273e-01,\n",
              "        -2.02188573e-01, -4.08758940e-01],\n",
              "       [-1.43558691e-02,  8.95650532e-02,  5.11771028e-01,\n",
              "        -2.25918191e-01,  4.33462931e-01, -8.30224876e-02,\n",
              "         2.90530938e-01, -1.69595710e-01],\n",
              "       [-2.95908221e-02,  2.09381656e-01,  9.00363920e-01,\n",
              "        -6.39115495e-01,  1.15375607e+00, -7.74803135e-01,\n",
              "         3.48389107e+00, -4.11005689e-01],\n",
              "       [ 4.63685963e-01, -4.62858294e-02,  2.33301850e-01,\n",
              "         5.20769796e-02,  3.28270458e-01, -2.08904746e-01,\n",
              "        -1.73804955e-01, -3.79484226e-01],\n",
              "       [-3.19796930e-01, -6.53812153e-02,  2.84676915e-01,\n",
              "        -5.24473258e-01,  5.08444435e-01,  1.20449149e-01,\n",
              "        -2.18505395e-01, -3.94964221e-01],\n",
              "       [-7.03012711e-02,  1.14784595e-01,  7.43683277e-01,\n",
              "        -2.12302865e-01,  3.43826306e-01,  1.77416254e-01,\n",
              "        -1.66552915e-01, -4.16783260e-01],\n",
              "       [-6.15888911e-02,  3.30579484e-01,  9.00306385e-01,\n",
              "        -8.26984982e-01,  1.26473596e+00, -1.07190060e+00,\n",
              "        -1.13686447e-01,  1.82124250e+00],\n",
              "       [-3.02140915e-01, -1.14279840e-01,  3.13777679e-01,\n",
              "        -6.22612095e-01,  7.50557487e-01, -8.68400817e-02,\n",
              "        -8.03936548e-01,  4.47981508e-01],\n",
              "       [-3.27897402e-01, -5.79115630e-02,  4.23558914e-01,\n",
              "        -5.28389284e-01,  5.37397449e-01,  2.45108988e-01,\n",
              "        -2.44586825e-01, -4.48795825e-01],\n",
              "       [-4.47726349e-01,  2.34538090e+00,  1.36792226e-01,\n",
              "        -2.99653106e-01, -2.25022523e-01, -8.72707787e-01,\n",
              "         7.33752998e-02,  4.65338537e-02],\n",
              "       [-1.85521963e+00, -1.48259361e+00, -1.50375775e+00,\n",
              "         3.11683588e-01, -5.53424572e-01, -8.03383319e-01,\n",
              "         2.03426657e-01,  3.64674681e-01],\n",
              "       [-1.47575622e+00, -1.29893728e+00, -9.45272120e-01,\n",
              "         8.69250132e-01, -6.03979442e-01, -1.84456954e+00,\n",
              "         4.60146845e-02,  2.93279295e-01],\n",
              "       [-1.92301949e+00, -1.55233972e+00, -1.57079498e+00,\n",
              "         2.34902499e-01, -5.76501315e-01, -7.84916798e-01,\n",
              "         1.74863739e-01,  2.79125025e-01],\n",
              "       [ 1.19481190e+00, -1.66349587e-01, -1.57798411e-01,\n",
              "         1.03269117e-01,  1.00830136e-01, -3.02572694e-02,\n",
              "        -1.21991253e-01, -2.04145571e-01],\n",
              "       [ 1.97179432e+00, -3.35446403e-01, -5.01413313e-01,\n",
              "        -4.79573388e-01, -6.80082277e-01,  1.39222491e-02,\n",
              "         9.96270923e-02,  1.64093225e-01],\n",
              "       [ 2.18742931e+00, -3.86758635e-01, -6.29398813e-01,\n",
              "        -6.26524796e-01, -9.37171165e-01,  1.87259041e-02,\n",
              "         1.64244837e-01,  2.95144820e-01],\n",
              "       [-1.21306969e-02,  1.17730415e-01,  8.34249541e-01,\n",
              "        -6.65135619e-01,  1.21487014e+00, -6.21537398e-01,\n",
              "        -1.55408244e+00,  1.87021350e+00],\n",
              "       [-4.19785605e-01, -3.45774675e-01,  9.67099619e-01,\n",
              "         5.69370925e-01, -5.86143504e-01,  2.02956286e-01,\n",
              "         1.73168962e-01,  4.20912452e-01],\n",
              "       [-3.04256867e-01, -2.20302761e-01,  6.97966961e-01,\n",
              "         3.31198561e-02, -1.10272901e-01,  4.57880665e-02,\n",
              "        -1.45404713e-01, -4.03667512e-01],\n",
              "       [-3.04256867e-01, -2.20302761e-01,  6.97966961e-01,\n",
              "         3.31198561e-02, -1.10272901e-01,  4.57880665e-02,\n",
              "        -1.45404713e-01, -4.03667512e-01],\n",
              "       [-3.04256867e-01, -2.20302761e-01,  6.97966961e-01,\n",
              "         3.31198561e-02, -1.10272901e-01,  4.57880665e-02,\n",
              "        -1.45404713e-01, -4.03667512e-01],\n",
              "       [ 1.76226117e+00, -2.90677524e-01, -4.16480409e-01,\n",
              "        -3.40524312e-01, -4.98983072e-01,  8.15185891e-03,\n",
              "         5.52275489e-02,  9.49709882e-02],\n",
              "       [-5.12447298e-01,  2.54315497e+00, -3.88484017e-01,\n",
              "         8.52581358e-02, -6.60030000e-01, -2.45236801e-01,\n",
              "        -1.15031556e-01, -1.68268499e-01],\n",
              "       [ 9.66080648e-01, -1.24281881e-01,  1.05478897e-02,\n",
              "        -5.82060823e-01, -4.08252996e-01,  2.74012158e-02,\n",
              "         1.31416864e-01,  1.48309254e-01],\n",
              "       [ 2.18742931e+00, -3.86758635e-01, -6.29398813e-01,\n",
              "        -6.26524796e-01, -9.37171165e-01,  1.87259041e-02,\n",
              "         1.64244837e-01,  2.95144820e-01],\n",
              "       [ 1.86805512e+00, -3.11969328e-01, -4.40587985e-01,\n",
              "        -3.79919298e-01, -5.36062797e-01, -1.34003966e-02,\n",
              "         4.31627130e-02,  6.27076344e-02],\n",
              "       [-7.78402551e-01, -6.34000738e-01,  1.59735891e-01,\n",
              "        -3.60497244e-01, -3.59089651e-02, -1.85355251e-01,\n",
              "        -3.76818508e-01, -7.83339218e-01],\n",
              "       [-1.16953341e+00, -8.91434630e-01, -5.31412869e-01,\n",
              "        -5.54807351e-01, -8.65768518e-02,  2.07291265e-01,\n",
              "        -1.48761138e-01, -3.95285543e-01],\n",
              "       [-2.69627317e-01, -1.93174750e-01,  5.23895985e-01,\n",
              "         1.49904512e-02, -6.71696273e-02, -1.47057772e-02,\n",
              "        -7.65330626e-02, -2.14882365e-01],\n",
              "       [ 1.93053662e+00, -3.23331219e-01, -4.55822624e-01,\n",
              "        -3.65749628e-01, -5.21820560e-01, -1.88020754e-02,\n",
              "         1.62734425e-02,  1.63857292e-02],\n",
              "       [ 1.34151915e+00, -1.88895479e-01, -8.67248613e-01,\n",
              "         2.03693465e+00,  1.26634602e+00,  1.78820848e-01,\n",
              "        -7.30715201e-03, -9.09297525e-02],\n",
              "       [-8.18158694e-01, -6.75983819e-01,  1.74493149e-01,\n",
              "        -4.06937696e-01, -4.06619925e-02, -2.15822651e-01,\n",
              "        -4.51786712e-01, -9.43277143e-01],\n",
              "       [ 4.81050919e-02,  7.14684794e-02, -1.21832741e-01,\n",
              "         1.13193546e+00,  9.80642132e-01,  1.19368800e-01,\n",
              "         8.58968711e-02,  2.52674874e-02],\n",
              "       [-6.77822150e-01,  2.63307355e+00, -8.48750802e-01,\n",
              "         4.54016911e-01, -5.27108185e-01, -1.83288835e-02,\n",
              "         3.96163890e-02,  1.37538964e-01],\n",
              "       [-7.72132126e-03,  8.46692691e-02,  4.52639810e-01,\n",
              "        -1.56559935e-01,  3.29407554e-01, -4.42180485e-02,\n",
              "         2.11623902e-01, -1.68079515e-01],\n",
              "       [ 4.29429077e-01, -3.11555767e-03, -2.70247381e-01,\n",
              "         1.26735932e+00,  1.05406209e+00,  5.74935210e-02,\n",
              "        -5.61402495e-02, -2.47225650e-01],\n",
              "       [ 1.23144008e+00, -1.66126199e-01, -3.10038423e-01,\n",
              "         8.27307064e-01,  6.11551748e-01,  6.77809193e-02,\n",
              "        -1.04214623e-01, -2.20259408e-01],\n",
              "       [-8.11418598e-01,  3.09145035e-02, -7.53579365e-01,\n",
              "        -7.40557893e-01,  4.37737487e-01,  1.01237813e+00,\n",
              "         4.89409169e-02,  6.33423494e-02],\n",
              "       [-4.14106578e-01,  2.13637643e+00, -5.50064915e-03,\n",
              "         5.75164510e-02, -6.48683367e-01, -3.89838067e-01,\n",
              "        -1.91707823e-01, -3.47057142e-01],\n",
              "       [-3.88659110e-02,  2.32476242e-02,  5.97967771e-01,\n",
              "        -1.03411557e-01,  6.12621117e-01, -4.01609310e-01,\n",
              "        -6.71342137e-01, -1.44771622e+00],\n",
              "       [ 1.23595284e+00, -1.50634649e-01, -1.62999936e-01,\n",
              "         2.44434450e-01,  2.81261220e-01, -3.73069227e-01,\n",
              "        -6.45103847e-01, -3.29543969e-01],\n",
              "       [-3.75672542e-01,  2.14736502e+00, -1.11031673e-01,\n",
              "         1.38811068e-02, -5.54022452e-01, -5.13411455e-01,\n",
              "        -2.41022544e-01, -3.70395218e-01],\n",
              "       [-1.16035228e+00,  3.97214577e-01, -1.62387919e+00,\n",
              "        -5.15795305e-01,  6.79641200e-01,  1.91344087e+00,\n",
              "         3.19529854e-01,  6.04376407e-01],\n",
              "       [ 5.06495977e-01, -5.41311151e-02,  2.22644703e-01,\n",
              "         6.21628825e-02,  3.38424694e-01, -2.12812325e-01,\n",
              "        -1.93551518e-01, -4.13576947e-01],\n",
              "       [-1.32748250e+00, -1.05325964e+00, -7.25626101e-01,\n",
              "        -5.96018556e-01, -1.74344035e-01,  1.12345218e-01,\n",
              "        -1.43568014e-01, -4.19002564e-01],\n",
              "       [ 1.34151915e+00, -1.88895479e-01, -8.67248613e-01,\n",
              "         2.03693465e+00,  1.26634602e+00,  1.78820848e-01,\n",
              "        -7.30715201e-03, -9.09297525e-02]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indata=principalDf.to_numpy()\n",
        "indata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "SF5ZZH1LVDug",
        "outputId": "98c69f0d-9ec1-4a9c-b8e0-9cfef6b495f9"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-bfc1d35616aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \"\"\"\n\u001b[1;32m    418\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    389\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 274 and input n_features is 272 "
          ]
        }
      ],
      "source": [
        "clf.predict(sentence_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0FwRZaMZbSZ",
        "outputId": "658084e5-0869-4d27-8bed-0ccb14f7a1ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y=len(sentence)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJj-1Hi64Vuz"
      },
      "outputs": [],
      "source": [
        "#associative memory\n",
        "(x,)=numpy.shape(gram3)\n",
        "(y,y1)=numpy.shape(ypredbin)\n",
        "weight= numpy.ones((y1,x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZbHClPK9ubu",
        "outputId": "171ea83a-6543-4c45-93d2-e57c85edee5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(y,y1)=numpy.shape(ypredbin)\n",
        "y1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7BOiARe6Tzt"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(y):\n",
        "  for j in range(y1):\n",
        "    if ypredbin[i,j] ==1:\n",
        "      for k in range(x):\n",
        "        if sentence_vec[i,k]==0:\n",
        "          weight[j,k]=weight[j,k]-0.1*weight[j,k]\n",
        "    elif ypredbin[i,j] ==0:\n",
        "      for k in range(x):\n",
        "        if sentence_vec[i,k]==1:\n",
        "          weight[j,k]=weight[j,k]-0.1*weight[j,k]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ3q2e1r_pOG",
        "outputId": "696e63e6-2f89-47b9-f802-ff7d491b8623"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testinput=ypredbin[0,:]\n",
        "testoutput=sentence_vec[0,:]\n",
        "max=numpy.zeros((y1))\n",
        "index=numpy.zeros((y1),int)-1\n",
        "for i in range(y1):\n",
        "  if testinput[i]==1:\n",
        "    max[i]=0\n",
        "    index[i]=-1\n",
        "    for j in range(x):\n",
        "      if testoutput[j]==1:\n",
        "        if max[i]<weight[i,j]:\n",
        "          max[i]=weight[i,j]\n",
        "          index[i]=j\n",
        "result =numpy.argmax(max)\n",
        "\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8_yxppNFoHK",
        "outputId": "fac9ca63-0d8a-4bca-e490-3d0aa50b838f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index[result]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwpczUwDBUT9",
        "outputId": "45481949-e2e3-42c8-ddd1-27f0d724312d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}